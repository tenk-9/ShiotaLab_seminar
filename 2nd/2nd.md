# 第2回
- 10/18(水)
- 14:40~
## 内容
- STFTのパラメータを変更して，時間分解能と周波数分解能の変化を見る．
- 話者認識に関する記事を読んで，まとめる

## STFTのパラメータを変更して，時間分解能と周波数分解能の変化を見る
- `spectrogram.ipynb`
- 音声は「たったったーたたたたったー」
- シフト長は`窓サイズ/4`
- 2048点FFT
- 窓長LONGER
  - 周波数分解能UP
  - 時間分解能DOWN

## 話者認識に関する記事を読んで，まとめる
- https://www.jstage.jst.go.jp/article/jasj/69/7/69_KJ00008760764/_article/-char/ja/
### はじめに
話者認識を含めパターン認識の課題全般にわたって，機械学習の手法の有効性が確認されている．
- 機械学習はデータから帰納的に知見を抽出する方法や枠組み
- 大まかには4段階に分けられる
  - データから特徴量を抽出する
  - 特徴量をモデル化する
  - モデルのパラメータを推定するための評価基準
  - パラメータの最適化

モデル化の部分に焦点を当て，生成モデル，識別モデル，因子分析モデルの3つを比較する．
### 生成モデル
- 各話者の特徴量を生成する分布のモデル
  - >短時間フレームの特徴ベクトルが，ある話者を表す確率分布から生成されるという仮定に基づく
  - 音声データから特徴量ベクトルを得たとき，そのベクトルの各要素は確率分布に従うことを仮定する
    - 仮に特徴量を正規化した振幅スペクトルとしたら，ある人の発する声について，450Hzの成分は多い傾向にある，500Hzの成分は逆に少ないことが多い，みたいな
    - 特徴量としては，スペクトログラム，ケプストラム係数ベクトル列，ショートタイムエネルギーなど，様々
      - ケプストラム
        - >ケプストラム(cepstrum)とは，スペクトラム(spectrum) のアナグラムによる導出語であり， 音声波形をフーリエ変換して得たパワースペクトルについて， その値の対数をとり，さらに逆フーリエ変換した結果を指す．
        - https://vision.kuee.kyoto-u.ac.jp/lecture/dsp/dictionary/cepstrum.htm
      - ショートタイムエネルギー
        - 音声信号を短く分割したものについて，それぞれのエネルギーの計算
- 学習にあたっては，データから得られる特徴量ベクトル達を用いて，分布のパラメータを決定する
- 評価については，尤度(対数尤度)を用いることができる
  - 尤度: 各要素の確率の積
  - 対数尤度: log(尤度)，尤度の最大化と対数尤度の最大化は等価
#### GMM-UBM
- Gaussian mixture model - universal background model
  - GMM: 混合ガウス分布
    - >データが複数の正規分布(ガウス分布)から生成されたと考えて分析する手法([ref](https://nisshingeppo.com/ai/whats-gaussian-mixture-model/))
    - `mix_gauss.jpg`: 分布が組み合わさるイメージ
    - 実際は，各分布ごとに混合係数が存在する
      - 混合係数: 各分布の重要性や寄与度，総和が1
- 当該話者のデータのみを用いて学習するのではなく，一般的な音声らしいモデル(UBM)を当該話者の分布に適合させることでモデルを構築する
  - 不特定話者データを用いて一般的なモデル(UBM)を構築しておく
  - 当該話者のモデル構築の際は，その話者のデータを用いて分布のパラメータを調整する_?
- 照合に際しては，詐称者モデルと対象話者モデルの尤度比に基づく
  - チューニング前のUBMを詐称者モデルとする
- 登録(モデルの調整に用いる)データと評価データの音素に偏りがあると精度が下がる
  - >音素（おんそ、英: phoneme）とは、言語学・音韻論において、音声学的な違いはどうであれ、心理的な実在として、母語話者にとって同じと感じられ、また意味を区別する働きをする音声上の最小単位となる音韻的単位を指す。(WiKi)
  - この解消には登録データと評価データに多くのデータが必要
    - 音素や音節ごとに話者UBMを構築したり，音素分類木を用いる方法が提案されている
#### GMMスーパーベクトル
- GMMの平均ベクトルを結合して得られるベクトル
  - 近年はこれを特徴量として用いる考え方が主流
    - 二次元の分布で，各コンポーネントの平均ベクトルが(1,2),(3,4)のとき，GMMスーパーベクトルは((1,2)',(3,4)')のよう
  - 特徴量次元より高次元
  - 時系列データをベクトル空間上の1点で表せる
    - 一発話のデータでUBMを適応して得たGMMの平均ベクトルから，その発話データのGMMスーパーベクトルが得られる
  - SVM等の識別モデルの入力として用いられる
  - 因子分析モデルアプローチもスーパーベクトル空間上で実現されている
### 識別モデル
- カーネルマシンを用いたアプローチ
  - 特徴量空間を高次元空間に写像する
    - 写像の仕方はカーネル関数による
  - カーネル関数によるモデル，最大マージンや最尤の評価基準，最適化アルゴリズムが組み込まれている
    - SVM
      - Support Vector Machine
      - 最大マージンによる評価基準
    - dPLRM
      - dual penalized logistic regression machine
      - 最尤基準
  - 分布のモデルを仮定してパラメータを推定する訳ではないので，高次元の特徴量を扱える
  - どのようなカーネル関数を用いるか，どのような特徴量を用いるかが主な検討事項
    - カーネル関数
      - 系列(sequence)カーネル
      - GMMスーパーベクトルカーネル
      - 共分散カーネル
      - global alignment(GA)カーネル
    - 特徴量
      - 分析フレーム列
      - GMMスーパーベクトル
      - 対数パワースペクトル
  - 扱うデータに応じてカーネル関数やそのパラメータを適切に選択しないと性能が出ない
    - 複数のカーネル関数を凸結合するマルチカーネル学習が代表的なアプローチ
#### GMMスーパーベクトルカーネル
- GMMスーパーベクトル特徴量にあわせて考案された
- 特徴量がD次元，GMMのコンポーネント数がNのとき，GMMスーパーベクトルはD*N次元
  - 高次元空間に写像している
#### GAカーネル
- Global alignmentカーネル
- ベクトル時系列を引数に取る
  - 2つの音声それぞれに窓関数を適用し，特徴ベクトル時系列を得る
- 2つの時系列間のいろいろな非線形伸縮軸上での距離を求め，その距離を用いてカーネルの値を計算
  - DTW(dynamic time warping, 動的時間伸縮法)
    - 2つのデータについて，各データ点間の距離を総当たりで求める
    - 点数の多いデータ内の各点について，最短の距離の総和を距離の指標にする
    - 時系列方向の移動や伸縮に強い
    - https://qiita.com/kotaRof/items/af3320dfbed31a6dc4ac
    - https://en.wikipedia.org/wiki/Dynamic_time_warping
    - https://data-analysis-stats.jp/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92/dtwdynamic-time-warping%E5%8B%95%E7%9A%84%E6%99%82%E9%96%93%E4%BC%B8%E7%B8%AE%E6%B3%95/
### 因子分析モデル
スーパーベクトル空間上で表された発話データについて，話者以外の要因を除去しようとするアプローチ
- 録音環境等の影響を除去したい
- 一大トレンド
#### JFA
- 話者の声とチャネルの影響を明示的にモデル化する
- 話者内の音響特徴の変動(録音環境の違い，セッションによる変化)に強い
- GMMスーパーベクトルが，話者を表現するベクトル，チャネルを表現するベクトルに分解できることを仮定している
  - M: 発話から得たGMMスーパーベクトル
  - m: UBMから得られる話者及びチャネル非依存のスーパーベクトル
  - V: 話者部分空間を定義する行列
  - U: チャネル部分空間を表現する行列
  - D: 残差成分を定義する行列
- 大規模なラベル付き学習データでV,U,Dを推定
- 与えられた発話データでy,x,zを推定
- チャネル変動成分補正後の話者モデルに対して，入力発話の特徴ベクトルの尤度を計算し，評価する
- 音素変動のモデル化も試みられている
#### i-vector
因子分析では発話データを話者・チャネル依存の全変動空間に写像し，チャネル変動については別に除去するアプローチ
- JFAでは話者以外の要因をモデル化して除去することの限界がある
  - DehakがJFAによって得られたチャネル因子にも話者情報が含まれていることを示した
- M: 発話から抽出したGMMスーパーベクトル，話者とチャネルに依存している
- m: UBMから得られるGMMスーパーベクトル，話者とチャネルに非依存
- T: 低ランクの矩形行列，全変動空間を張る基底ベクトルから構成される
- w: 発話に対するi-vector
  - GMMスーパーベクトル空間における一般的な話者(UBM)からの差として話者を表現したもの
  - 話者とチャネル依存の部分空間上のベクトル
    - セッションごとの音響変動やチャネル差異の影響を低減する必要がある
  - 特徴量の次元削減も叶っており，うれしい
- チャネル変動補正の正確さが全体の性能の鍵
  - PLDAによるアプローチもチャネル変動補正の1方式といえる
- 入力データに対して得たi-vectorと，話者モデルとして登録したi-vectorのコサイン類似度によって照合する
#### PLDA
- Probabilistic Linear Discriminant Analysis
  - i-vector空間において直接的に話者変動やチャネル変動をモデル化する試み
  - w: 与えられた発話に対するi-vector
  - w_bar: i-vector空間におけるオフセット
  - PHI: 話者部分空間を張る基底行列
  - GAMMA: チャネル部分空間を張る基底行列
  - beta: 話者因子，正規分布に従う
  - alpha: チャネル因子，正規分布に従う
  - epsilon: 残差成分，平均ベクトル0，対角共分散行列SIGMAを持つ正規分布に従う
- これらの仮定の下にモデル化されたモデルをG-PLDA(Gaussian-)と呼ぶことがある
  - 仮定には疑問が残る部分もある
    - 話者成分とチャネル成分は統計的に独立である
    - 話者因子とチャネル因子はガウス分布に従う
      - i-vectorの非ガウス性を扱うため，Student-t分布を用いたPLDAが報告されている
        - G-PLDAより高性能
- 評価については，「2つのi-vectorが同一の話者モデルから生成されたか否か」について，対数尤度比を評価できる．
- 登録データ，評価データの発話継続長が短い場合，及び発話継続長にミスマッチがある場合は，PLDAをはじめとするアプローチは弱い

---
- DTW(dynamic time warping, 動的時間伸縮法)
  - 2つのデータについて，各データ点間の距離を総当たりで求める
  - 点数の多いデータ内の各点について，最短の距離の総和を距離の指標にする
  - 時系列方向の移動や伸縮に強い
  - https://qiita.com/kotaRof/items/af3320dfbed31a6dc4ac
  - https://en.wikipedia.org/wiki/Dynamic_time_warping
  - https://data-analysis-stats.jp/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92/dtwdynamic-time-warping%E5%8B%95%E7%9A%84%E6%99%82%E9%96%93%E4%BC%B8%E7%B8%AE%E6%B3%95/

